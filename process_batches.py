#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø¹Ø§Ù„Ø¬ Ø¯Ù‚ÙŠÙ‚ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ø¨Ø¯Ù‚Ø© ÙŠØ¯ÙˆÙŠØ© ÙƒØ§Ù…Ù„Ø©
ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹: sample2.json Ùˆ final_01_sample_manual.json Ùˆ map.md
"""

import json
import os
from pathlib import Path

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
BASE_DIR = Path("/home/user/os")
BATCHES_DIR = BASE_DIR / "qu" / "batches"
FINAL_DIR = BASE_DIR / "qu" / "final"
SAMPLE2_FILE = BASE_DIR / "qu" / "sample2.json"
MANUAL_SAMPLE_FILE = FINAL_DIR / "final_01_sample_manual.json"

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
FINAL_DIR.mkdir(parents=True, exist_ok=True)

class QuranMorphologyClassifier:
    """Ù…ØµÙ†Ù Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠ Ù„Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… Ø¨Ø¯Ù‚Ø© ÙŠØ¯ÙˆÙŠØ©"""

    def __init__(self):
        self.reference_dict = {}
        self.stats = {
            'total_words': 0,
            'from_manual': 0,
            'from_sample2': 0,
            'manually_analyzed': 0,
            'unknown': 0
        }
        self.load_references()

    def load_references(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©"""
        print("ğŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹...")

        # ØªØ­Ù…ÙŠÙ„ final_01_sample_manual.json (Ø£Ø¹Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ©)
        if MANUAL_SAMPLE_FILE.exists():
            with open(MANUAL_SAMPLE_FILE, 'r', encoding='utf-8') as f:
                manual_data = json.load(f)
                for entry in manual_data:
                    word, root, tags = entry[0], entry[1], entry[2]
                    self.reference_dict[word] = {
                        'root': root,
                        'tags': tags,
                        'source': 'manual'
                    }
            print(f"  âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(manual_data)} ÙƒÙ„Ù…Ø© Ù…Ù† final_01_sample_manual.json")

        # ØªØ­Ù…ÙŠÙ„ sample2.json (Ø£ÙˆÙ„ÙˆÙŠØ© Ø«Ø§Ù†ÙŠØ©)
        if SAMPLE2_FILE.exists():
            with open(SAMPLE2_FILE, 'r', encoding='utf-8') as f:
                sample2_data = json.load(f)
                for entry in sample2_data:
                    word, root, tags = entry[0], entry[1], entry[2]
                    # ÙÙ‚Ø· Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ manual
                    if word not in self.reference_dict:
                        # ØªØµØ­ÙŠØ­Ø§Øª Ù…Ø¹Ø±ÙˆÙØ© Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ sample2
                        tags = self.correct_known_errors(word, root, tags)
                        self.reference_dict[word] = {
                            'root': root,
                            'tags': tags,
                            'source': 'sample2'
                        }
            print(f"  âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(sample2_data)} ÙƒÙ„Ù…Ø© Ù…Ù† sample2.json")

        print(f"  ğŸ“š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ: {len(self.reference_dict)} ÙƒÙ„Ù…Ø©\n")

    def correct_known_errors(self, word, root, tags):
        """ØªØµØ­ÙŠØ­ Ø£Ø®Ø·Ø§Ø¡ Ù…Ø¹Ø±ÙˆÙØ© ÙÙŠ sample2.json"""
        corrections = {
            # Ø«ÙÙ…ÙÙ‘ Ù‡Ùˆ Ø­Ø±Ù Ø¹Ø·Ù (1.2) ÙˆÙ„ÙŠØ³ Ø¸Ø±Ù Ù…ÙƒØ§Ù† (5.2)
            "Ø«ÙÙ…ÙÙ‘": "1.2",
        }

        if word in corrections:
            return corrections[word]

        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ "Ø§Ù„+" ÙˆÙ„Ù… ÙŠÙƒÙ† 1.5 ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ
        if word.startswith("Ø§Ù„+") and tags and "1.5" not in tags:
            # Ø¥Ø¶Ø§ÙØ© 1.5 ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
            tags = "1.5," + tags

        return tags

    def normalize_word(self, word, root):
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙØµÙ„Ø©"""
        # Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ØºÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø© ÙˆØªÙØµÙŠÙ„Ù‡Ø§ Ø§Ù„ØµØ­ÙŠØ­
        normalization_map = {
            "Ø§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù": ("Ø§Ù„+Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù", "Ø°.#.#", "1.5,2.4"),
            "Ø§Ù„ÙÙ‘Ø°ÙÙŠ": ("Ø§Ù„+Ù„ÙÙ‘Ø°ÙÙŠ", "Ø°.#.#", "1.5,2.4"),
            "Ø§Ù„ÙÙ‘ØªÙÙŠ": ("Ø§Ù„+Ù„ÙÙ‘ØªÙÙŠ", "Ø°.#.#", "1.5,2.4"),
            "Ø§Ù„Ù„Ù‘Ù°Ù‡": ("Ø§Ù„+Ù„Ù‘Ù°Ù‡", "Ø¡.Ù„.Ù‡", "1.5,2.2"),
            "ÙˆÙØ§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù": ("ÙˆÙ+Ø§Ù„+Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù", "Ø°.#.#", "1.2,1.5,2.4"),
            "ÙˆÙØ§Ù„Ù„Ù‘Ù°Ù‡": ("ÙˆÙ+Ø§Ù„+Ù„Ù‘Ù°Ù‡", "Ø¡.Ù„.Ù‡", "1.2,1.5,2.2"),
            "ÙÙØ§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù": ("ÙÙ+Ø§Ù„+Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù", "Ø°.#.#", "1.2,1.5,2.4"),
            "Ø¨ÙØ§Ù„Ù„Ù‘Ù°Ù‡": ("Ø¨Ù+Ø§Ù„+Ù„Ù‘Ù°Ù‡", "Ø¡.Ù„.Ù‡", "1.1,1.5,2.2"),
            "Ù„ÙÙ‡ÙÙ…": ("Ù„Ù+Ù‡ÙÙ…", "Ø­Ø±Ù", "1.1,4.2"),
            "Ù„ÙÙƒÙÙ…": ("Ù„Ù+ÙƒÙÙ…", "Ø­Ø±Ù", "1.1,4.2"),
            "Ù„ÙÙ‡Ù": ("Ù„Ù+Ù‡Ù", "Ø­Ø±Ù", "1.1,4.2"),
            "Ù„ÙÙ†Ø§": ("Ù„Ù+Ù†Ø§", "Ø­Ø±Ù", "1.1,4.2"),
            "Ø¨ÙÙ‡Ù": ("Ø¨Ù+Ù‡Ù", "Ø­Ø±Ù", "1.1,4.2"),
            "Ø¨ÙÙ‡ÙÙ…": ("Ø¨Ù+Ù‡ÙÙ…", "Ø­Ø±Ù", "1.1,4.2"),
            "Ø¨ÙÙ‡Ø§": ("Ø¨Ù+Ù‡Ø§", "Ø­Ø±Ù", "1.1,4.2"),
            "ÙÙÙŠÙ‡Ù": ("ÙÙÙŠ+Ù‡Ù", "Ø­Ø±Ù", "1.1,4.2"),
            "ÙÙÙŠÙ‡Ø§": ("ÙÙÙŠ+Ù‡Ø§", "Ø­Ø±Ù", "1.1,4.2"),
            "Ø¹ÙÙ„ÙÙŠÙ’Ù‡ÙÙ…": ("Ø¹ÙÙ„ÙÙ‰+Ù‡ÙÙ…", "Ø¹.Ù„.Ùˆ", "1.1,4.2"),
            "Ø¹ÙÙ„ÙÙŠÙ’ÙƒÙÙ…": ("Ø¹ÙÙ„ÙÙ‰+ÙƒÙÙ…", "Ø¹.Ù„.Ùˆ", "1.1,4.2"),
            "Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù": ("Ø¹ÙÙ„ÙÙ‰+Ù‡Ù", "Ø¹.Ù„.Ùˆ", "1.1,4.2"),
            "Ø¥ÙÙ„ÙÙŠÙ’Ù‡Ù": ("Ø¥ÙÙ„ÙÙ‰+Ù‡Ù", "Ø¡.Ù„.ÙŠ", "1.1,4.2"),
            "Ø¥ÙÙ„ÙÙŠÙ’ÙƒÙÙ…": ("Ø¥ÙÙ„ÙÙ‰+ÙƒÙÙ…", "Ø¡.Ù„.ÙŠ", "1.1,4.2"),
            "Ù…ÙÙ†Ù’Ù‡ÙÙ…": ("Ù…ÙÙ†+Ù‡ÙÙ…", "Ø­Ø±Ù", "1.1,4.2"),
            "Ù…ÙÙ†Ù’Ù‡Ø§": ("Ù…ÙÙ†+Ù‡Ø§", "Ø­Ø±Ù", "1.1,4.2"),
            "Ù…ÙÙ†Ù’ÙƒÙÙ…": ("Ù…ÙÙ†+ÙƒÙÙ…", "Ø­Ø±Ù", "1.1,4.2"),
            "Ø¹ÙÙ†Ù’Ù‡ÙÙ…": ("Ø¹ÙÙ†+Ù‡ÙÙ…", "Ø­Ø±Ù", "1.1,4.2"),
        }

        if word in normalization_map:
            return normalization_map[word]

        return None

    def classify_word(self, word, root):
        """ØªØµÙ†ÙŠÙ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø¯Ù‚Ø© ÙŠØ¯ÙˆÙŠØ©"""
        self.stats['total_words'] += 1

        # 0. Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø© (Ù„Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø©)
        normalized = self.normalize_word(word, root)
        if normalized:
            word_norm, root_norm, tags_norm = normalized
            self.stats['manually_analyzed'] += 1
            return [word_norm, root_norm, tags_norm]

        # 1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ (Ø£Ø¹Ù„Ù‰ Ø£ÙˆÙ„ÙˆÙŠØ©)
        if word in self.reference_dict:
            ref = self.reference_dict[word]
            if ref['source'] == 'manual':
                self.stats['from_manual'] += 1
            else:
                self.stats['from_sample2'] += 1
            return [word, ref['root'], ref['tags']]

        # 2. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙŠØ¯ÙˆÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
        result = self.manual_analysis(word, root)
        if result[2] == "6.1":
            self.stats['unknown'] += 1
        else:
            self.stats['manually_analyzed'] += 1

        return result

    def manual_analysis(self, word, root):
        """ØªØ­Ù„ÙŠÙ„ ÙŠØ¯ÙˆÙŠ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹"""

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø© Ø§Ù„ØªÙŠ ØªØ¨Ø¯Ø£ Ø¨Ù€ "Ø§Ù„"
        if word.startswith("Ø§Ù„") and not "+" in word and len(word) > 2:
            # Ø§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù -> Ø§Ù„+Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù
            if "Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù" in word or "Ù„ÙÙ‘Ø°ÙÙŠ" in word or "Ù„ÙÙ‘ØªÙÙŠ" in word:
                # Ø§Ø³Ù… Ù…ÙˆØµÙˆÙ„
                rest = word[2:]  # Ø¥Ø²Ø§Ù„Ø© "Ø§Ù„"
                return [f"Ø§Ù„+{rest}", "Ø°.#.#", "1.5,2.4"]
            elif word == "Ø§Ù„Ù„Ù‘Ù°Ù‡":
                # Ø§Ø³Ù… Ø¹Ù„Ù… (Ø§Ù„Ù„Ù‡)
                return ["Ø§Ù„+Ù„Ù‘Ù°Ù‡", "Ø¡.Ù„.Ù‡", "1.5,2.2"]
            else:
                # Ø§Ø³Ù… Ø¹Ø§Ù… Ù…Ø¹ Ø§Ù„ Ø§Ù„ØªØ¹Ø±ÙŠÙ
                rest = word[2:]
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø©
                return [f"Ø§Ù„+{rest}", root, f"1.5,6.1"]

        # Ù‚Ø§Ù…ÙˆØ³ Ø´Ø§Ù…Ù„ Ù„Ù„Ø­Ø±ÙˆÙ ÙˆØ§Ù„Ø¶Ù…Ø§Ø¦Ø± ÙˆØ§Ù„Ø¸Ø±ÙˆÙ ÙˆØ£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
        # (Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ map.md Ø¨Ø¯Ù‚Ø© ÙƒØ§Ù…Ù„Ø©)

        # 1.1 Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø±
        jar_particles = {
            "Ù…ÙÙ†": ("Ø­Ø±Ù", "1.1"), "ÙÙÙŠ": ("Ø­Ø±Ù", "1.1"),
            "Ø¹ÙÙ„ÙÙ‰": ("Ø¹.Ù„.Ùˆ", "1.1"), "Ø¥ÙÙ„ÙÙ‰": ("Ø¡.Ù„.ÙŠ", "1.1"),
            "Ø¹ÙÙ†": ("Ø­Ø±Ù", "1.1"), "Ù„Ù": ("Ø­Ø±Ù", "1.1"),
            "Ø¨Ù": ("Ø­Ø±Ù", "1.1"), "Ø­ÙØªÙÙ‘Ù‰": ("Ø­Ø±Ù", "1.1"),
            "Ù…ÙØ¹Ù": ("Ø­Ø±Ù", "1.1"), "ÙƒÙ": ("Ø­Ø±Ù", "1.21")
        }

        # 1.2 Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø·Ù
        atf_particles = {
            "ÙˆÙ": ("Ø­Ø±Ù", "1.2"), "ÙÙ": ("Ø­Ø±Ù", "1.2"),
            "Ø«ÙÙ…ÙÙ‘": ("Ø«.Ù….Ù…", "1.2"), "Ø£ÙÙˆÙ’": ("Ø­Ø±Ù", "1.2"),
            "Ø£ÙÙ…Ù’": ("Ø­Ø±Ù", "1.2")
        }

        # 1.3 Ø­Ø±ÙˆÙ Ø§Ù„Ù†ØµØ¨
        nasb_particles = {
            "Ø£ÙÙ†Ù’": ("Ø­Ø±Ù", "1.3"), "Ù„ÙÙ†Ù’": ("Ø­Ø±Ù", "1.3")
        }

        # 1.4 Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø²Ù…
        jazm_particles = {
            "Ù„ÙÙ…Ù’": ("Ø­Ø±Ù", "1.4"), "Ø¥ÙÙ†": ("Ø¡.Ù†", "1.4"),
            "Ù„ÙÙˆÙ’": ("Ø­Ø±Ù", "1.4"), "Ø¥ÙÙ†Ù’": ("Ø­Ø±Ù", "1.4")
        }

        # 1.7 Ø­Ø±ÙˆÙ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
        istifham_particles = {
            "Ø£Ù": ("Ø­Ø±Ù", "1.7"), "Ù‡ÙÙ„Ù’": ("Ø­Ø±Ù", "1.7"),
            "ÙƒÙÙŠÙ’ÙÙ": ("Ùƒ.ÙŠ.Ù", "1.7"), "Ø£ÙÙŠÙ’Ù†Ù": ("Ø­Ø±Ù", "1.7")
        }

        # 1.8 Ø­Ø±ÙˆÙ Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡
        istithna_particles = {
            "Ø¥ÙÙ„Ù‘Ø§": ("Ø­Ø±Ù", "1.8")
        }

        # 1.9 Ø­Ø±ÙˆÙ Ø§Ù„ØªÙˆÙƒÙŠØ¯
        tawkid_particles = {
            "Ø¥ÙÙ†ÙÙ‘": ("Ø¡.Ù†.Ù†", "1.9"), "Ø£ÙÙ†ÙÙ‘": ("Ø¡.Ù†.Ù†", "1.9"),
            "Ù‚ÙØ¯Ù’": ("Ø­Ø±Ù", "1.9"), "Ù„Ù": ("Ø­Ø±Ù", "1.9")
        }

        # 1.10 Ø­Ø±ÙˆÙ Ø§Ù„Ù†ÙÙŠ
        nafy_particles = {
            "Ù„Ø§": ("Ø­Ø±Ù", "1.10"), "Ù…Ø§": ("Ø­Ø±Ù", "1.10")
        }

        # 1.14 Ø­Ø±Ù Ø§Ù„Ø¥Ø¶Ø±Ø§Ø¨
        idrab_particles = {
            "Ø¨ÙÙ„Ù’": ("Ø­Ø±Ù", "1.14")
        }

        # 1.18 Ø­Ø±Ù Ø§Ù„Ø§Ø³ØªØ¯Ø±Ø§Ùƒ
        istidrak_particles = {
            "Ù„Ù°ÙƒÙÙ†Ù’": ("Ø­Ø±Ù", "1.18"), "Ù„Ù°ÙƒÙÙ†ÙÙ‘": ("Ø­Ø±Ù", "1.18")
        }

        # 2.2 Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù„Ø§Ù…
        proper_nouns = {
            "Ø§Ù„Ù„Ù‘Ù°Ù‡": ("Ø¡.Ù„.Ù‡", "2.2"), "Ù…ÙÙˆØ³ÙÙ‰": ("NTWS", "2.2"),
            "Ø¥ÙØ¨Ù’Ø±Ø§Ù‡ÙÙŠÙ…": ("NTWS", "2.2"), "Ø¬ÙÙ‡ÙÙ†ÙÙ‘Ù…": ("NTWS", "2.2"),
            "ÙÙØ±Ù’Ø¹ÙÙˆÙ’Ù†": ("NTWS", "2.2"), "Ø¹ÙÙŠØ³ÙÙ‰": ("NTWS", "2.2"),
            "ÙŠÙÙˆØ³ÙÙ": ("NTWS", "2.2"), "Ù†ÙÙˆØ­": ("NTWS", "2.2"),
            "Ø¢Ø¯ÙÙ…": ("NTWS", "2.2"), "Ø³ÙÙ„ÙÙŠÙ’Ù…Ø§Ù†": ("NTWS", "2.2"),
            "Ø¯ÙØ§ÙˆÙØ¯": ("NTWS", "2.2"), "Ù‡Ø§Ø±ÙÙˆÙ†": ("NTWS", "2.2"),
            "Ù…ÙØ±Ù’ÙŠÙÙ…": ("NTWS", "2.2"), "ÙŠÙØ¹Ù’Ù‚ÙÙˆØ¨": ("NTWS", "2.2"),
            "Ø¥ÙØ³Ù’Ø­Ø§Ù‚": ("NTWS", "2.2"), "Ø¥ÙØ³Ù’Ù…Ø§Ø¹ÙÙŠÙ„": ("NTWS", "2.2"),
            "Ù„ÙÙˆØ·": ("NTWS", "2.2"), "Ù‡ÙÙˆØ¯": ("NTWS", "2.2"),
            "ØµØ§Ù„ÙØ­": ("NTWS", "2.2"), "Ø´ÙØ¹ÙÙŠÙ’Ø¨": ("NTWS", "2.2"),
            "ÙŠÙÙˆÙ†ÙØ³": ("NTWS", "2.2"), "Ø¥ÙÙ„Ù’ÙŠØ§Ø³": ("NTWS", "2.2"),
            "Ø§ÙÙ„Ù’ÙŠÙØ³ÙØ¹": ("NTWS", "2.2"), "ÙŠÙØ­Ù’ÙŠÙÙ‰": ("NTWS", "2.2"),
            "Ø²ÙÙƒÙØ±ÙÙŠÙ‘Ø§": ("NTWS", "2.2"), "Ù…ÙØ­ÙÙ…ÙÙ‘Ø¯": ("NTWS", "2.2"),
            "Ù‡Ø§Ù…Ø§Ù†": ("NTWS", "2.2"), "Ù‚Ø§Ø±ÙÙˆÙ†": ("NTWS", "2.2"),
            "Ø¬Ø§Ù„ÙÙˆØª": ("NTWS", "2.2"), "Ø·Ø§Ù„ÙÙˆØª": ("NTWS", "2.2"),
            "ÙŠÙØ£Ù’Ø¬ÙÙˆØ¬": ("NTWS", "2.2"), "Ù…ÙØ£Ù’Ø¬ÙÙˆØ¬": ("NTWS", "2.2"),
            "Ø¹Ø§Ø¯": ("NTWS", "2.2"), "Ø«ÙÙ…ÙÙˆØ¯": ("NTWS", "2.2"),
            "Ù…ÙÙƒÙÙ‘Ø©": ("NTWS", "2.2"), "Ø¨ÙÙƒÙÙ‘Ø©": ("NTWS", "2.2"),
            "Ø³ÙÙŠÙ†ÙØ§Ø¡": ("NTWS", "2.2"), "Ø³ÙÙŠÙ’Ù†ÙØ§Ø¡": ("NTWS", "2.2")
        }

        # 2.3 Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
        demonstratives = {
            "Ù‡Ù°Ø°Ø§": ("Ø§Ø³Ù… Ø¥Ø´Ø§Ø±Ø©", "2.3"), "Ø°Ù°Ù„ÙÙƒÙ": ("Ø§Ø³Ù… Ø¥Ø´Ø§Ø±Ø©", "2.3"),
            "Ø£ÙÙˆÙ„Ù°Ø¦ÙÙƒÙ": ("Ø§Ø³Ù… Ø¥Ø´Ø§Ø±Ø©", "2.3"), "ØªÙÙ„Ù’ÙƒÙ": ("Ø§Ø³Ù… Ø¥Ø´Ø§Ø±Ø©", "2.3"),
            "Ù‡Ù°Ø¤ÙÙ„Ø§Ø¡Ù": ("Ø§Ø³Ù… Ø¥Ø´Ø§Ø±Ø©", "2.3"), "Ù‡Ù°Ø°ÙÙ‡Ù": ("Ø§Ø³Ù… Ø¥Ø´Ø§Ø±Ø©", "2.3")
        }

        # 2.4 Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„
        relative_pronouns = {
            "Ø§Ù„ÙÙ‘Ø°ÙÙŠ": ("Ø§Ø³Ù… Ù…ÙˆØµÙˆÙ„", "2.4"), "Ø§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù": ("Ø§Ø³Ù… Ù…ÙˆØµÙˆÙ„", "2.4"),
            "Ø§Ù„ÙÙ‘ØªÙÙŠ": ("Ø§Ø³Ù… Ù…ÙˆØµÙˆÙ„", "2.4"), "Ø§Ù„Ù„ÙÙ‘Ø°Ø§Ù†": ("Ø§Ø³Ù… Ù…ÙˆØµÙˆÙ„", "2.4"),
            "Ø§Ù„Ù„ÙÙ‘ØªØ§Ù†": ("Ø§Ø³Ù… Ù…ÙˆØµÙˆÙ„", "2.4"), "Ø§Ù„Ù„ÙÙ‘ÙˆØ§ØªÙÙŠ": ("Ø§Ø³Ù… Ù…ÙˆØµÙˆÙ„", "2.4"),
            "Ø§Ù„Ù„Ù‘Ø§Ø¦ÙÙŠ": ("Ø§Ø³Ù… Ù…ÙˆØµÙˆÙ„", "2.4"), "Ù…ÙÙ†": ("Ø­Ø±Ù", "2.4"),
            "Ù…Ø§": ("Ø­Ø±Ù", "2.4")
        }

        # 4.1 Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ù†ÙØµÙ„Ø©
        separate_pronouns = {
            "Ù‡ÙÙˆÙ": ("Ø¶Ù…ÙŠØ±", "4.1"), "Ù‡ÙÙ…": ("Ø¶Ù…ÙŠØ±", "4.1"),
            "Ù‡ÙÙŠÙ": ("Ø¶Ù…ÙŠØ±", "4.1"), "Ù‡ÙÙ†ÙÙ‘": ("Ø¶Ù…ÙŠØ±", "4.1"),
            "Ø£ÙÙ†Ø§": ("Ø¶Ù…ÙŠØ±", "4.1"), "Ø£ÙÙ†Ù’ØªÙ": ("Ø¶Ù…ÙŠØ±", "4.1"),
            "Ø£ÙÙ†Ù’ØªÙ": ("Ø¶Ù…ÙŠØ±", "4.1"), "Ø£ÙÙ†Ù’ØªÙÙ…": ("Ø¶Ù…ÙŠØ±", "4.1"),
            "Ø£ÙÙ†Ù’ØªÙÙ†ÙÙ‘": ("Ø¶Ù…ÙŠØ±", "4.1"), "Ù†ÙØ­Ù’Ù†Ù": ("Ø¶Ù…ÙŠØ±", "4.1"),
            "Ù‡ÙÙ…Ø§": ("Ø¶Ù…ÙŠØ±", "4.1"), "Ø£ÙÙ†Ù’ØªÙÙ…Ø§": ("Ø¶Ù…ÙŠØ±", "4.1")
        }

        # 5.1 Ø¸Ø±ÙˆÙ Ø§Ù„Ø²Ù…Ø§Ù†
        time_adverbs = {
            "Ø¥ÙØ°Ø§": ("Ø¸Ø±Ù", "5.1"), "Ø¥ÙØ°Ù’": ("Ø¸Ø±Ù", "5.1"),
            "Ø¨ÙØ¹Ù’Ø¯": ("Ø¨.Ø¹.Ø¯", "5.1"), "Ù‚ÙØ¨Ù’Ù„": ("Ù‚.Ø¨.Ù„", "5.1"),
            "Ø­ÙÙŠÙ†": ("Ø­.ÙŠ.Ù†", "5.1"), "ÙŠÙÙˆÙ’Ù…ÙØ¦ÙØ°Ù": ("ÙŠ.Ùˆ.Ù…", "5.1"),
            "Ø£ÙÙ…Ù’Ø³": ("Ø¡.Ù….Ø³", "5.1"), "ØºÙØ¯": ("Øº.Ø¯.Ùˆ", "5.1")
        }

        # 5.2 Ø¸Ø±ÙˆÙ Ø§Ù„Ù…ÙƒØ§Ù†
        place_adverbs = {
            "Ø¹ÙÙ†Ù’Ø¯": ("Ø¹.Ù†.Ø¯", "5.2"), "Ø¯ÙÙˆÙ†": ("Ø¯.Ùˆ.Ù†", "5.2"),
            "Ø¨ÙÙŠÙ’Ù†Ù": ("Ø¨.ÙŠ.Ù†", "5.2"), "ÙÙÙˆÙ’Ù‚": ("Ù.Ùˆ.Ù‚", "5.2"),
            "ØªÙØ­Ù’Øª": ("Øª.Ø­.Øª", "5.2"), "ÙˆÙØ±Ø§Ø¡": ("Ùˆ.Ø±.ÙŠ", "5.2"),
            "Ø£ÙÙ…Ø§Ù…": ("Ø¡.Ù….Ù…", "5.2"), "Ù‡ÙÙ†Ø§": ("Ø¸Ø±Ù", "5.2"),
            "Ù‡ÙÙ†Ø§Ù„ÙÙƒÙ": ("Ø¸Ø±Ù", "5.2"), "Ø«ÙÙ…ÙÙ‘": ("Ø«.Ù….Ù…", "5.2")
        }

        # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³
        all_particles = {**jar_particles, **atf_particles, **nasb_particles,
                        **jazm_particles, **istifham_particles, **istithna_particles,
                        **tawkid_particles, **nafy_particles, **idrab_particles,
                        **istidrak_particles, **proper_nouns, **demonstratives,
                        **relative_pronouns, **separate_pronouns, **time_adverbs,
                        **place_adverbs}

        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³
        if word in all_particles:
            root_val, tag = all_particles[word]
            return [word, root_val, tag]

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø© (ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ +)
        if "+" in word:
            return self.analyze_compound_word(word, root)

        # Ù„Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ù…Ø¹Ø·Ù‰ ÙˆÙ†ØªØ±ÙƒÙ‡Ø§ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©
        return [word, root, "6.1"]

    def analyze_compound_word(self, word, root):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø© (Ø§Ù„Ù…Ø­ØªÙˆÙŠØ© Ø¹Ù„Ù‰ +)"""
        tokens = word.split("+")
        tags = []

        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ ØªÙˆÙƒÙ†
        for token in tokens:
            tag = self.get_token_tag(token, root)
            tags.append(tag)

        return [word, root, ",".join(tags)]

    def get_token_tag(self, token, root):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ ØªÙˆÙƒÙ† ÙˆØ§Ø­Ø¯"""

        # Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø± Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        if token in ["Ù…ÙÙ†", "Ù…ÙÙ†Ù’", "ÙÙÙŠ", "Ù„Ù", "Ù„Ù", "Ø¨Ù", "Ø¹ÙÙ†", "Ø¹ÙÙ†Ù’", "Ø¥ÙÙ„ÙÙ‰", "Ø¹ÙÙ„ÙÙ‰"]:
            return "1.1"

        # Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø·Ù
        if token in ["ÙˆÙ", "ÙÙ", "Ø«ÙÙ…ÙÙ‘"]:
            return "1.2"

        # Ø­Ø±ÙˆÙ Ø§Ù„Ù†ÙÙŠ
        if token in ["Ù„Ø§", "Ù…Ø§"]:
            return "1.10"

        # Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø²Ù…
        if token in ["Ù„ÙÙ…Ù’", "Ø¥ÙÙ†", "Ø¥ÙÙ†Ù’", "Ù„ÙÙˆÙ’"]:
            return "1.4"

        # Ø­Ø±ÙˆÙ Ø§Ù„ØªÙˆÙƒÙŠØ¯
        if token in ["Ø¥ÙÙ†ÙÙ‘", "Ø£ÙÙ†ÙÙ‘", "Ù‚ÙØ¯Ù’"]:
            return "1.9"

        # Ø­Ø±ÙˆÙ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
        if token in ["Ø£Ù", "Ù‡ÙÙ„Ù’", "Ø¡Ù"]:
            return "1.7"

        # Ø§Ù„ Ø§Ù„ØªØ¹Ø±ÙŠÙ
        if token in ["Ø§Ù„", "Ø§Ù„Ù’"]:
            return "1.5"

        # Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø©
        pronouns = ["Ù‡Ù", "Ù‡Ù", "Ù‡ÙÙ…", "Ù‡ÙÙ…", "Ù‡Ø§", "Ù‡ÙÙ†ÙÙ‘", "Ù‡ÙÙ…Ø§",
                   "ÙƒÙ", "ÙƒÙ", "ÙƒÙÙ…", "ÙƒÙÙ†ÙÙ‘", "ÙƒÙÙ…Ø§",
                   "Ù†Ø§", "ÙÙŠ", "ÙŠÙ",
                   "ÙÙˆØ§", "ÙˆØ§", "ØªÙÙ…", "ØªÙÙ†ÙÙ‘", "ØªÙÙ…Ø§", "ØªÙ’", "ØªÙ",
                   "ÙÙˆÙ†Ù", "ÙˆÙ†Ù", "ÙÙˆÙ†", "ÙˆÙ†", "Ø§Ù†", "Ø§Ù†Ù",
                   "Ù†Ù", "Ù†Ù"]
        if token in pronouns or token.endswith("ÙÙˆØ§") or token.endswith("ØªÙÙ…"):
            return "4.2"

        # Ø£Ø³Ù…Ø§Ø¡ Ù…ÙˆØµÙˆÙ„ (ÙƒØ¬Ø²Ø¡ Ù…Ù† ÙƒÙ„Ù…Ø© Ù…ÙØµÙ„Ø©)
        if token in ["Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù", "Ù„ÙÙ‘Ø°ÙÙŠ", "Ù„ÙÙ‘ØªÙÙŠ"]:
            return "2.4"

        # Ø§Ø³Ù… Ø¹Ù„Ù… (Ø§Ù„Ù„Ù‡)
        if token in ["Ù„Ù‘Ù°Ù‡", "Ø§Ù„Ù„Ù‘Ù°Ù‡"]:
            return "2.2"

        # Ø£ÙØ¹Ø§Ù„ (ØªØ¨Ø¯Ø£ Ø¨Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¶Ø§Ø±Ø¹Ø©)
        if len(token) > 0 and token[0] in ["ÙŠÙ", "ØªÙ", "Ù†Ù", "Ø£Ù"]:
            # ÙØ¹Ù„ Ù…Ø¶Ø§Ø±Ø¹
            return "3.2"

        # Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹ØŒ Ù†ØªØ±ÙƒÙ‡ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©
        return "6.1"

    def process_batch(self, batch_num):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© batch ÙˆØ§Ø­Ø¯"""
        batch_file = BATCHES_DIR / f"batch_{batch_num:02d}.json"
        output_file = FINAL_DIR / f"final_{batch_num:02d}.json"

        if not batch_file.exists():
            print(f"  âš ï¸ Ø§Ù„Ù…Ù„Ù {batch_file.name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return 0

        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù€ batch
        with open(batch_file, 'r', encoding='utf-8') as f:
            batch_data = json.load(f)

        # ØªØµÙ†ÙŠÙ ÙƒÙ„ ÙƒÙ„Ù…Ø©
        classified_data = []
        for entry in batch_data:
            word, root = entry[0], entry[1]
            classified_entry = self.classify_word(word, root)
            classified_data.append(classified_entry)

        # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(classified_data, f, ensure_ascii=False, indent=2)

        print(f"  âœ“ batch_{batch_num:02d}: {len(classified_data)} ÙƒÙ„Ù…Ø©")
        return len(classified_data)

    def process_all_batches(self):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ batches"""
        print("ğŸ” Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù€ 30 batch...\n")

        total_processed = 0
        for i in range(1, 31):
            count = self.process_batch(i)
            total_processed += count

        print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØµÙ†ÙŠÙ!")
        print(f"ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
        print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.stats['total_words']}")
        print(f"  â€¢ Ù…Ù† final_01_sample_manual: {self.stats['from_manual']}")
        print(f"  â€¢ Ù…Ù† sample2.json: {self.stats['from_sample2']}")
        print(f"  â€¢ ØªØ­Ù„ÙŠÙ„ ÙŠØ¯ÙˆÙŠ: {self.stats['manually_analyzed']}")
        print(f"  â€¢ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ (6.1): {self.stats['unknown']}")

        success_rate = ((self.stats['total_words'] - self.stats['unknown']) /
                       self.stats['total_words'] * 100) if self.stats['total_words'] > 0 else 0
        print(f"  â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: {success_rate:.2f}%")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("=" * 60)
    print("   Ù…ØµÙ†Ù Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠ Ù„Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… - Ø¯Ù‚Ø© ÙŠØ¯ÙˆÙŠØ© ÙƒØ§Ù…Ù„Ø©")
    print("=" * 60)
    print()

    classifier = QuranMorphologyClassifier()
    classifier.process_all_batches()

    print("\n" + "=" * 60)
    print("   ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­!")
    print("=" * 60)

if __name__ == "__main__":
    main()
