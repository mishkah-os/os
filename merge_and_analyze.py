#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ø³ÙƒØ±ÙŠØ¨Øª Ø¯Ù…Ø¬ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„
Merge all final_XX.json files with words-ref.json and perform comprehensive analysis
"""

import json
import sys
from pathlib import Path
from collections import Counter, defaultdict
from typing import List, Dict, Tuple

class QuranMergeAnalyzer:
    def __init__(self, data_dir='qu'):
        self.data_dir = Path(data_dir)
        self.merged_data = []
        self.statistics = {}

    def load_and_merge(self):
        """Load all final files and merge with reference data"""
        print("=" * 80)
        print("ğŸ”„ Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠ")
        print("=" * 80)

        # 1. Load words-ref.json
        print("\nğŸ“‚ ØªØ­Ù…ÙŠÙ„ words-ref.json...")
        ref_file = self.data_dir / 'words-ref.json'
        with open(ref_file, 'r', encoding='utf-8') as f:
            words_ref = json.load(f)
        print(f"âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(words_ref)} Ù…ÙˆØ¶Ø¹ Ù…Ø±Ø¬Ø¹ÙŠ")

        # 2. Load all final_XX.json files
        print("\nğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª final_XX.json...")
        all_morphology = []
        for i in range(1, 16):
            final_file = self.data_dir / 'final' / f'final_{i:02d}.json'
            if not final_file.exists():
                print(f"âš  Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {final_file}")
                continue

            with open(final_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_morphology.extend(data)
                print(f"  âœ“ final_{i:02d}.json: {len(data)} ÙƒÙ„Ù…Ø©")

        print(f"\nâœ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©: {len(all_morphology)} ÙƒÙ„Ù…Ø©")

        # 3. Merge data
        print("\nğŸ”— Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

        # Flatten words_ref to get sequential word positions
        word_positions = []
        for ref_group in words_ref:
            if isinstance(ref_group, list) and len(ref_group) > 0:
                if isinstance(ref_group[0], list):
                    # First element is a list of positions
                    word_positions.extend(ref_group[0])
                else:
                    # Single position
                    word_positions.append(ref_group)

        print(f"âœ“ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹: {len(word_positions)}")
        print(f"âœ“ Ø¹Ø¯Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©: {len(all_morphology)}")

        # Check if counts match
        if len(word_positions) != len(all_morphology):
            print(f"\nâš  ØªØ­Ø°ÙŠØ±: Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯!")
            print(f"  Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹: {len(word_positions)}")
            print(f"  Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©: {len(all_morphology)}")
            # Use minimum length to avoid index errors
            min_length = min(len(word_positions), len(all_morphology))
            print(f"  Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…: {min_length} ÙƒÙ„Ù…Ø©")
        else:
            min_length = len(all_morphology)
            print(f"âœ“ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„: {min_length} ÙƒÙ„Ù…Ø©")

        # Merge
        for i in range(min_length):
            position = word_positions[i] if i < len(word_positions) else [0, 0, 0]
            morphology = all_morphology[i] if i < len(all_morphology) else ["", "", ""]

            # Extract position info
            if isinstance(position, list) and len(position) == 3:
                surah, ayah, word_idx = position
            else:
                surah, ayah, word_idx = 0, 0, 0

            # Extract morphology info
            if isinstance(morphology, list) and len(morphology) >= 3:
                word, root, tags = morphology[0], morphology[1], morphology[2]
            else:
                word, root, tags = "", "", ""

            merged_entry = {
                'index': i + 1,
                'surah': surah,
                'ayah': ayah,
                'word_position': word_idx,
                'word': word,
                'root': root,
                'tags': tags
            }
            self.merged_data.append(merged_entry)

        print(f"\nâœ“ ØªÙ… Ø¯Ù…Ø¬ {len(self.merged_data)} ÙƒÙ„Ù…Ø©")

        # Save merged data
        output_file = self.data_dir / 'merged_quran_full.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.merged_data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©: {output_file}")

        return True

    def analyze_statistics(self):
        """Perform comprehensive statistical analysis"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„")
        print("=" * 80)

        # Basic counts
        word_freq = Counter()
        root_freq = Counter()
        tag_freq = Counter()
        morpheme_freq = Counter()

        # Surah statistics
        surah_word_count = Counter()
        surah_root_diversity = defaultdict(set)

        # Tag co-occurrence
        tag_cooccurrence = defaultdict(Counter)

        # Root by tag
        root_by_tag = defaultdict(Counter)
        tag_by_root = defaultdict(Counter)

        print("\nğŸ”¢ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª...")

        for entry in self.merged_data:
            word = entry['word']
            root = entry['root']
            tags = entry['tags']
            surah = entry['surah']

            # Word frequency
            word_freq[word] += 1

            # Root frequency
            if root and root != 'NTWS':
                root_freq[root] += 1

            # Surah statistics
            surah_word_count[surah] += 1
            if root and root != 'NTWS':
                surah_root_diversity[surah].add(root)

            # Tag statistics
            if tags:
                tag_list = tags.split(',')
                for tag in tag_list:
                    tag = tag.strip()
                    tag_freq[tag] += 1

                    if root:
                        root_by_tag[tag][root] += 1
                        tag_by_root[root][tag] += 1

                # Tag co-occurrence
                for i, tag1 in enumerate(tag_list):
                    for tag2 in tag_list[i+1:]:
                        tag_cooccurrence[tag1.strip()][tag2.strip()] += 1

            # Morpheme frequency
            if '+' in word:
                morphemes = word.split('+')
                for morpheme in morphemes:
                    morpheme_freq[morpheme] += 1

        # Store statistics
        self.statistics = {
            'total_words': len(self.merged_data),
            'unique_words': len(word_freq),
            'unique_roots': len(root_freq),
            'unique_tags': len(tag_freq),
            'total_surahs': max(surah_word_count.keys()) if surah_word_count else 0,
            'word_freq': dict(word_freq.most_common(100)),
            'root_freq': dict(root_freq.most_common(100)),
            'tag_freq': dict(tag_freq.most_common()),
            'morpheme_freq': dict(morpheme_freq.most_common(50)),
            'surah_word_count': dict(surah_word_count),
            'surah_root_diversity': {k: len(v) for k, v in surah_root_diversity.items()},
            'root_by_tag': {k: dict(v.most_common(10)) for k, v in root_by_tag.items()},
            'tag_by_root': {k: dict(v) for k, v in list(tag_by_root.items())[:50]},
            'tag_cooccurrence': {k: dict(v.most_common(10)) for k, v in list(tag_cooccurrence.items())[:20]}
        }

        print(f"\nâœ“ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:")
        print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {self.statistics['total_words']:,}")
        print(f"  â€¢ ÙƒÙ„Ù…Ø§Øª ÙØ±ÙŠØ¯Ø©: {self.statistics['unique_words']:,}")
        print(f"  â€¢ Ø¬Ø°ÙˆØ± ÙØ±ÙŠØ¯Ø©: {self.statistics['unique_roots']:,}")
        print(f"  â€¢ ØªØµÙ†ÙŠÙØ§Øª ÙØ±ÙŠØ¯Ø©: {self.statistics['unique_tags']}")
        print(f"  â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙˆØ±: {self.statistics['total_surahs']}")

        # Save statistics
        stats_file = self.data_dir / 'analysis_statistics.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.statistics, f, ensure_ascii=False, indent=2)
        print(f"\nâœ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {stats_file}")

    def generate_text_report(self):
        """Generate detailed text report"""
        print("\n" + "=" * 80)
        print("ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØµÙŠ")
        print("=" * 80)

        report = []
        report.append("=" * 80)
        report.append("ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„")
        report.append("Comprehensive Analysis Report - Complete Quranic Text")
        report.append("=" * 80)
        report.append("")

        # Basic statistics
        report.append("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©")
        report.append("-" * 80)
        report.append(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {self.statistics['total_words']:,}")
        report.append(f"ÙƒÙ„Ù…Ø§Øª ÙØ±ÙŠØ¯Ø©: {self.statistics['unique_words']:,}")
        report.append(f"Ø¬Ø°ÙˆØ± ÙØ±ÙŠØ¯Ø©: {self.statistics['unique_roots']:,}")
        report.append(f"ØªØµÙ†ÙŠÙØ§Øª Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©: {self.statistics['unique_tags']}")
        report.append(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙˆØ±: {self.statistics['total_surahs']}")
        report.append("")

        # Most frequent words
        report.append("ğŸ”¤ Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹ (Top 20)")
        report.append("-" * 80)
        for i, (word, count) in enumerate(list(self.statistics['word_freq'].items())[:20], 1):
            report.append(f"{i:2d}. {word:30s} {count:6,} Ù…Ø±Ø©")
        report.append("")

        # Most frequent roots
        report.append("ğŸŒ± Ø£ÙƒØ«Ø± Ø§Ù„Ø¬Ø°ÙˆØ± ØªÙƒØ±Ø§Ø±Ø§Ù‹ (Top 20)")
        report.append("-" * 80)
        for i, (root, count) in enumerate(list(self.statistics['root_freq'].items())[:20], 1):
            report.append(f"{i:2d}. {root:20s} {count:6,} Ù…Ø±Ø©")
        report.append("")

        # Tag distribution
        report.append("ğŸ· ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©")
        report.append("-" * 80)
        for tag, count in sorted(self.statistics['tag_freq'].items(), key=lambda x: x[1], reverse=True):
            percentage = (count / self.statistics['total_words']) * 100
            report.append(f"{tag:10s} {count:6,} ({percentage:5.2f}%)")
        report.append("")

        # Surah statistics
        report.append("ğŸ“– Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø³ÙˆØ± (Top 20 Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª)")
        report.append("-" * 80)
        surah_stats = sorted(self.statistics['surah_word_count'].items(),
                            key=lambda x: x[1], reverse=True)[:20]
        for surah, word_count in surah_stats:
            root_diversity = self.statistics['surah_root_diversity'].get(surah, 0)
            report.append(f"Ø§Ù„Ø³ÙˆØ±Ø© {surah:3d}: {word_count:5,} ÙƒÙ„Ù…Ø©ØŒ {root_diversity:4d} Ø¬Ø°Ø± ÙØ±ÙŠØ¯")
        report.append("")

        report_text = "\n".join(report)

        # Save report
        report_file = self.data_dir / 'analysis_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)

        print(f"âœ“ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_file}")
        print("\n" + report_text)

    def run(self):
        """Run complete analysis pipeline"""
        print("\n" + "=" * 80)
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ")
        print("=" * 80)

        # Step 1: Merge data
        if not self.load_and_merge():
            print("âŒ ÙØ´Ù„ ÙÙŠ Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            return False

        # Step 2: Analyze
        self.analyze_statistics()

        # Step 3: Generate report
        self.generate_text_report()

        print("\n" + "=" * 80)
        print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­")
        print("=" * 80)

        return True


if __name__ == '__main__':
    analyzer = QuranMergeAnalyzer()
    success = analyzer.run()
    sys.exit(0 if success else 1)
