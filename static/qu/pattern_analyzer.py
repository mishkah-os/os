#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ù†Ù…Ø· Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
Advanced Pattern Extraction and Statistical Analysis
"""

import json
import os
from collections import defaultdict, Counter
from pathlib import Path

class PatternAnalyzer:
    def __init__(self, data_dir='qu'):
        self.data_dir = data_dir
        self.final_data = []
        self.words_ref = []
        self.statistics = {}
        self.patterns = {}

    def load_data(self):
        """Load all morphological data from final/*.json files"""
        print("ğŸ“‚ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©...")

        final_dir = Path(self.data_dir) / 'final'
        for i in range(1, 16):
            filename = final_dir / f'final_{i:02d}.json'
            if filename.exists():
                with open(filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.final_data.extend(data)
                    print(f"  âœ“ ØªØ­Ù…ÙŠÙ„ {filename.name}: {len(data)} ÙƒÙ„Ù…Ø©")

        # Load words references
        ref_file = Path(self.data_dir) / 'words-ref.json'
        if ref_file.exists():
            with open(ref_file, 'r', encoding='utf-8') as f:
                self.words_ref = json.load(f)
                print(f"âœ“ ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹: {len(self.words_ref)} Ø¥Ø¯Ø®Ø§Ù„Ø©")

        print(f"\nâœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.final_data)} ÙƒÙ„Ù…Ø© Ø¨Ù†Ø¬Ø§Ø­\n")
        return len(self.final_data) > 0

    def calculate_statistics(self):
        """Calculate comprehensive statistics"""
        print("ğŸ“Š Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª...")

        word_freq = Counter()
        root_freq = Counter()
        tag_freq = Counter()
        morpheme_freq = Counter()
        root_by_tag = defaultdict(lambda: defaultdict(int))

        for word, root, tags in self.final_data:
            word_freq[word] += 1
            if root and root != 'NTWS':
                root_freq[root] += 1

            if tags:
                tag_list = tags.split(',')
                for tag in tag_list:
                    tag = tag.strip()
                    tag_freq[tag] += 1
                    if root:
                        root_by_tag[tag][root] += 1

            # Count morpheme frequency
            if '+' in word:
                morphemes = word.split('+')
                for morpheme in morphemes:
                    morpheme_freq[morpheme] += 1

        self.statistics = {
            'total_words': len(self.final_data),
            'unique_words': len(word_freq),
            'unique_roots': len(root_freq),
            'unique_tags': len(tag_freq),
            'word_frequency': word_freq,
            'root_frequency': root_freq,
            'tag_frequency': tag_freq,
            'morpheme_frequency': morpheme_freq,
            'root_by_tag': dict(root_by_tag)
        }

        print(f"  âœ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {self.statistics['total_words']}")
        print(f"  âœ“ ÙƒÙ„Ù…Ø§Øª ÙØ±ÙŠØ¯Ø©: {self.statistics['unique_words']}")
        print(f"  âœ“ Ø¬Ø°ÙˆØ± ÙØ±ÙŠØ¯Ø©: {self.statistics['unique_roots']}")
        print(f"  âœ“ ØªØµÙ†ÙŠÙØ§Øª ÙØ±ÙŠØ¯Ø©: {self.statistics['unique_tags']}\n")

    def discover_patterns(self):
        """Discover statistical patterns in the data"""
        print("ğŸ¯ Ø¬Ø§Ø±ÙŠ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©...\n")

        word_freq = self.statistics['word_frequency']
        root_freq = self.statistics['root_frequency']
        tag_freq = self.statistics['tag_frequency']

        # 1. Most frequent words
        top_words = word_freq.most_common(20)
        self.patterns['top_words'] = {
            'name': 'Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹',
            'data': top_words,
            'description': 'Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ¸Ù‡Ø± Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø±Ø© ÙÙŠ Ø§Ù„Ù†Øµ'
        }

        # 2. Most frequent roots
        top_roots = root_freq.most_common(20)
        self.patterns['top_roots'] = {
            'name': 'Ø£ÙƒØ«Ø± Ø§Ù„Ø¬Ø°ÙˆØ± ØªÙƒØ±Ø§Ø±Ø§Ù‹',
            'data': top_roots,
            'description': 'Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø«Ù„Ø§Ø«ÙŠØ© ÙˆØ§Ù„Ø±Ø¨Ø§Ø¹ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹'
        }

        # 3. Most frequent tags
        top_tags = tag_freq.most_common(20)
        self.patterns['top_tags'] = {
            'name': 'Ø£ÙƒØ«Ø± Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹',
            'data': top_tags,
            'description': 'Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (Ø£Ø³Ù…Ø§Ø¡ØŒ Ø£ÙØ¹Ø§Ù„ØŒ Ø­Ø±ÙˆÙØŒ Ø¥Ù„Ø®)'
        }

        # 4. Distribution analysis
        distribution = self._analyze_distribution()
        self.patterns['distribution'] = distribution

        # 5. Word frequency patterns
        rare_words = [w for w, f in word_freq.items() if f == 1]
        common_words = [w for w, f in word_freq.items() if f > 10]

        self.patterns['frequency_distribution'] = {
            'name': 'ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª',
            'rare_words': len(rare_words),
            'common_words': len(common_words),
            'average_frequency': sum(word_freq.values()) / len(word_freq),
            'median_frequency': sorted(word_freq.values())[len(word_freq)//2]
        }

        # 6. Root-Tag correlations
        root_tag_correlation = self._calculate_correlations()
        self.patterns['root_tag_correlation'] = root_tag_correlation

        # 7. Morpheme patterns
        morpheme_freq = self.statistics['morpheme_frequency']
        top_morphemes = morpheme_freq.most_common(15)
        self.patterns['morpheme_patterns'] = {
            'name': 'Ø£ÙƒØ«Ø± Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹',
            'data': top_morphemes,
            'description': 'Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹'
        }

        print("âœ… ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø¨Ù†Ø¬Ø§Ø­\n")

    def _analyze_distribution(self):
        """Analyze distribution across Quran structure"""
        word_freq = self.statistics['word_frequency']

        frequency_ranges = {
            '1': 0,      # Once
            '2-5': 0,    # 2-5 times
            '6-10': 0,   # 6-10 times
            '11-20': 0,  # 11-20 times
            '21-50': 0,  # 21-50 times
            '50+': 0     # More than 50
        }

        for freq in word_freq.values():
            if freq == 1:
                frequency_ranges['1'] += 1
            elif 2 <= freq <= 5:
                frequency_ranges['2-5'] += 1
            elif 6 <= freq <= 10:
                frequency_ranges['6-10'] += 1
            elif 11 <= freq <= 20:
                frequency_ranges['11-20'] += 1
            elif 21 <= freq <= 50:
                frequency_ranges['21-50'] += 1
            else:
                frequency_ranges['50+'] += 1

        return {
            'name': 'ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª',
            'ranges': frequency_ranges,
            'description': 'ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ¸Ù‡Ø± Ù…Ø±Ø©ØŒ Ù…Ø±ØªÙŠÙ†ØŒ Ø§Ù„Ø®'
        }

    def _calculate_correlations(self):
        """Calculate correlations between roots and tags"""
        correlations = []
        root_by_tag = self.statistics['root_by_tag']

        for tag, roots in root_by_tag.items():
            total = sum(roots.values())
            top_roots = sorted(roots.items(), key=lambda x: x[1], reverse=True)[:3]
            correlations.append({
                'tag': tag,
                'top_roots': top_roots,
                'total_count': total
            })

        return sorted(correlations, key=lambda x: x['total_count'], reverse=True)[:15]

    def analyze_location_patterns(self):
        """Analyze patterns in word locations across Quran"""
        print("ğŸ—ºï¸  Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù…ØµØ­Ù...\n")

        if not self.words_ref:
            print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ§Ù‚Ø¹ Ù…ØªØ§Ø­Ø©\n")
            return

        sura_distribution = defaultdict(int)
        word_location_count = defaultdict(int)

        # Analyze location data
        for locations in self.words_ref[:min(100, len(self.words_ref))]:
            if isinstance(locations, list):
                for location in locations:
                    if len(location) >= 1:
                        sura = location[0]
                        sura_distribution[sura] += 1

        # Count distribution
        location_frequencies = Counter()
        for locations in self.words_ref:
            location_count = len(locations) if isinstance(locations, list) else 1
            location_frequencies[location_count] += 1

        self.patterns['location_distribution'] = {
            'name': 'ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù…ØµØ­Ù',
            'sura_count': len(sura_distribution),
            'location_frequency': dict(location_frequencies),
            'most_distributed_suras': sorted(
                sura_distribution.items(),
                key=lambda x: x[1],
                reverse=True
            )[:10]
        }

        print("âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹\n")

    def generate_report(self, output_file='pattern_analysis_report.txt'):
        """Generate comprehensive analysis report"""
        print(f"ğŸ“ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„...\n")

        report = []
        report.append("=" * 80)
        report.append("ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„")
        report.append("Comprehensive Morphological Statistical Analysis Report")
        report.append("=" * 80)
        report.append("")

        # Section 1: Overview
        report.append("\n" + "â”" * 80)
        report.append("1ï¸âƒ£ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Data Overview")
        report.append("â”" * 80)
        report.append(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª:           {self.statistics['total_words']:,}")
        report.append(f"ÙƒÙ„Ù…Ø§Øª ÙØ±ÙŠØ¯Ø©:             {self.statistics['unique_words']:,}")
        report.append(f"Ø¬Ø°ÙˆØ± ÙØ±ÙŠØ¯Ø©:              {self.statistics['unique_roots']:,}")
        report.append(f"ØªØµÙ†ÙŠÙØ§Øª Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©:      {self.statistics['unique_tags']:,}")

        freq_dist = self.statistics.get('frequency_distribution', {})
        if 'average_frequency' in freq_dist:
            report.append(f"Ù…ØªÙˆØ³Ø· ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø©:       {freq_dist['average_frequency']:.2f}")
            report.append(f"Ø§Ù„ÙˆØ³ÙŠØ·:                  {freq_dist['median_frequency']}")

        # Section 2: Top Words
        report.append("\n" + "â”" * 80)
        report.append("2ï¸âƒ£ Ø£ÙƒØ«Ø± 20 ÙƒÙ„Ù…Ø© ØªÙƒØ±Ø§Ø±Ø§Ù‹ - Top 20 Most Frequent Words")
        report.append("â”" * 80)
        for rank, (word, count) in enumerate(self.patterns['top_words']['data'], 1):
            percentage = (count / self.statistics['total_words']) * 100
            report.append(f"{rank:2d}. {word:20s} - {count:5d} Ù…Ø±Ø© ({percentage:5.2f}%)")

        # Section 3: Top Roots
        report.append("\n" + "â”" * 80)
        report.append("3ï¸âƒ£ Ø£ÙƒØ«Ø± 20 Ø¬Ø°Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹ - Top 20 Most Frequent Roots")
        report.append("â”" * 80)
        for rank, (root, count) in enumerate(self.patterns['top_roots']['data'], 1):
            percentage = (count / self.statistics['total_words']) * 100
            report.append(f"{rank:2d}. {root:15s} - {count:5d} Ù…Ø±Ø© ({percentage:5.2f}%)")

        # Section 4: Tag Distribution
        report.append("\n" + "â”" * 80)
        report.append("4ï¸âƒ£ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª - Tag Distribution")
        report.append("â”" * 80)
        for rank, (tag, count) in enumerate(self.patterns['top_tags']['data'], 1):
            percentage = (count / self.statistics['total_words']) * 100
            report.append(f"{rank:2d}. {tag:15s} - {count:5d} Ù…Ø±Ø© ({percentage:5.2f}%)")

        # Section 5: Frequency Analysis
        report.append("\n" + "â”" * 80)
        report.append("5ï¸âƒ£ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª - Frequency Distribution Analysis")
        report.append("â”" * 80)
        dist = self.patterns['distribution']['ranges']
        for range_key, count in dist.items():
            percentage = (count / self.statistics['unique_words']) * 100
            report.append(f"  {range_key:8s} - {count:5d} ÙƒÙ„Ù…Ø© ({percentage:5.2f}%)")

        # Section 6: Morpheme Patterns
        report.append("\n" + "â”" * 80)
        report.append("6ï¸âƒ£ Ø£ÙƒØ«Ø± Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹ - Top Morphemes")
        report.append("â”" * 80)
        for rank, (morpheme, count) in enumerate(self.patterns['morpheme_patterns']['data'], 1):
            percentage = (count / self.statistics['total_words']) * 100
            report.append(f"{rank:2d}. {morpheme:20s} - {count:5d} Ù…Ø±Ø© ({percentage:5.2f}%)")

        # Section 7: Root-Tag Correlations (Top 10)
        report.append("\n" + "â”" * 80)
        report.append("7ï¸âƒ£ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„ØªØµÙ†ÙŠÙØ§Øª - Root-Tag Correlations")
        report.append("â”" * 80)
        for rank, corr in enumerate(self.patterns['root_tag_correlation'][:10], 1):
            report.append(f"\n{rank}. Ø§Ù„ØªØµÙ†ÙŠÙ: {corr['tag']} (Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {corr['total_count']})")
            report.append(f"   Ø£ÙƒØ«Ø± Ø§Ù„Ø¬Ø°ÙˆØ±:")
            for root, count in corr['top_roots']:
                report.append(f"     - {root:15s}: {count:5d} Ù…Ø±Ø©")

        # Section 8: Location Patterns (if available)
        if 'location_distribution' in self.patterns:
            report.append("\n" + "â”" * 80)
            report.append("8ï¸âƒ£ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù…ØµØ­Ù - Location Distribution")
            report.append("â”" * 80)
            loc_data = self.patterns['location_distribution']
            report.append(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙˆØ± Ø§Ù„ØªÙŠ ØªÙˆØ¬Ø¯ ÙÙŠÙ‡Ø§ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {loc_data.get('sura_count', 'N/A')}")
            if 'most_distributed_suras' in loc_data:
                report.append("\nØ£ÙƒØ«Ø± Ø§Ù„Ø³ÙˆØ± ØªÙˆØ²ÙŠØ¹Ø§Ù‹:")
                for rank, (sura, count) in enumerate(loc_data['most_distributed_suras'][:10], 1):
                    report.append(f"  {rank:2d}. Ø§Ù„Ø³ÙˆØ±Ø© {sura}: {count} ÙƒÙ„Ù…Ø©")

        # Section 9: Statistical Insights
        report.append("\n" + "â”" * 80)
        report.append("9ï¸âƒ£ Ø±Ø¤Ù‰ ÙˆØªØ­Ù„ÙŠÙ„Ø§Øª Ø¥Ø­ØµØ§Ø¦ÙŠØ© - Statistical Insights")
        report.append("â”" * 80)

        top_word = self.patterns['top_words']['data'][0]
        report.append(f"\nğŸ“ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹: '{top_word[0]}' ({top_word[1]} Ù…Ø±Ø©)")

        top_root = self.patterns['top_roots']['data'][0]
        report.append(f"ğŸ“ Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹: '{top_root[0]}' ({top_root[1]} Ù…Ø±Ø©)")

        rare = self.patterns['frequency_distribution']['rare_words']
        report.append(f"ğŸ“ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø© (ØªØ¸Ù‡Ø± Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©): {rare}")

        common = self.patterns['frequency_distribution']['common_words']
        report.append(f"ğŸ“ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© (ØªØ¸Ù‡Ø± Ø£ÙƒØ«Ø± Ù…Ù† 10 Ù…Ø±Ø§Øª): {common}")

        # Section 10: Key Findings
        report.append("\n" + "â”" * 80)
        report.append("ğŸ”‘ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© - Key Findings")
        report.append("â”" * 80)

        report.append(f"""
âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ {self.statistics['total_words']:,} ÙƒÙ„Ù…Ø© Ù…ÙˆØ²Ø¹Ø© Ø¹Ù„Ù‰ {self.statistics['unique_words']:,} ÙƒÙ„Ù…Ø© ÙØ±ÙŠØ¯Ø©

âœ… Ø§Ù„Ù†ØµÙˆØµ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {self.statistics['unique_roots']:,} Ø¬Ø°Ø± Ù…Ø®ØªÙ„ÙØŒ Ù…Ù…Ø§ ÙŠØ¯Ù„ Ø¹Ù„Ù‰
   Ø«Ø±Ø§Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ ÙˆØ§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ø¬Ù…ÙŠ

âœ… ØªÙˆØ¬Ø¯ {self.statistics['unique_tags']:,} ØªØµÙ†ÙŠÙ Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠ Ù…Ø®ØªÙ„Ù

âœ… ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙŠØªØ¨Ø¹ Ù†Ù…Ø· Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ø­ÙŠØ«:
   - Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø©
   - Ø¹Ø¯Ø¯ Ù…ØªÙˆØ³Ø· Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
   - Ø¹Ø¯Ø¯ ØµØºÙŠØ± Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¬Ø¯Ø§Ù‹ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©

âœ… Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ© ØªØ´ÙŠØ± Ø¥Ù„Ù‰:
   - Ø§Ø®ØªÙŠØ§Ø± Ø¯Ù‚ÙŠÙ‚ Ù„Ù„ÙƒÙ„Ù…Ø§Øª
   - ØªÙˆØ§Ø²Ù†Ø§Øª Ù„ØºÙˆÙŠØ© Ù…ØªØ¹Ù…Ø¯Ø©
   - ØªÙƒØ±Ø§Ø±Ø§Øª Ø°Ø§Øª Ù…Ø¹Ù†Ù‰ (ÙˆÙ„ÙŠØ³Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©)
        """)

        report.append("\n" + "=" * 80)
        report.append("Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªÙ‚Ø±ÙŠØ± - End of Report")
        report.append("=" * 80)

        # Write to file
        report_text = "\n".join(report)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_text)

        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {output_file}\n")

        # Also print to console
        print(report_text)

        return report_text

    def run_analysis(self):
        """Run complete analysis pipeline"""
        print("\n" + "=" * 80)
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©")
        print("=" * 80 + "\n")

        if not self.load_data():
            print("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            return False

        self.calculate_statistics()
        self.discover_patterns()
        self.analyze_location_patterns()
        self.generate_report()

        return True


if __name__ == '__main__':
    analyzer = PatternAnalyzer()
    analyzer.run_analysis()
