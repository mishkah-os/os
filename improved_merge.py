#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ÿßŸÑÿØŸÖÿ¨ ÿßŸÑŸÖÿ≠ÿ≥ŸëŸÜ ŸÖÿπ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖŸàÿßŸÇÿπ
Improved Merge with All Positions
"""

import json
from pathlib import Path

class ImprovedMerger:
    def __init__(self, data_dir='qu'):
        self.data_dir = Path(data_dir)

    def merge_with_all_positions(self):
        """Merge with all positions for each word"""
        print("=" * 80)
        print("üîó ÿßŸÑÿØŸÖÿ¨ ÿßŸÑŸÖÿ≠ÿ≥ŸëŸÜ - ŸÉŸÑ ÿßŸÑŸÖŸàÿßŸÇÿπ")
        print("=" * 80)

        # 1. Load final data
        print("\n1Ô∏è‚É£ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖŸàÿ±ŸÅŸàŸÑŸàÿ¨Ÿäÿ©...")
        all_morphology = []
        for i in range(1, 16):
            file_path = self.data_dir / 'final' / f'final_{i:02d}.json'
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_morphology.extend(data)

        print(f"   ‚úì ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ {len(all_morphology):,} ŸÉŸÑŸÖÿ© ŸÅÿ±ŸäÿØÿ©")

        # 2. Load ref data
        print("\n2Ô∏è‚É£ ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖŸàÿßŸÇÿπ...")
        with open(self.data_dir / 'words-ref.json', 'r', encoding='utf-8') as f:
            words_ref = json.load(f)

        print(f"   ‚úì ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ {len(words_ref):,} ÿπŸÜÿµÿ±")

        # 3. Parse positions from ref
        print("\n3Ô∏è‚É£ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖŸàÿßŸÇÿπ...")
        all_positions = []
        total_positions = 0

        for item in words_ref:
            if isinstance(item, list) and len(item) > 0:
                text = item[0]
                # Parse positions string
                try:
                    positions_str = '[' + text + ']'
                    positions = json.loads(positions_str)
                    all_positions.append(positions)
                    total_positions += len(positions)
                except:
                    all_positions.append([])
            else:
                all_positions.append([])

        print(f"   ‚úì ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑŸÖŸàÿßŸÇÿπ: {total_positions:,}")

        # 4. Merge
        print("\n4Ô∏è‚É£ ÿßŸÑÿØŸÖÿ¨...")
        merged_data = []

        for i in range(len(all_morphology)):
            word, root, tags = all_morphology[i]
            positions = all_positions[i] if i < len(all_positions) else []

            entry = {
                'index': i + 1,
                'word': word,
                'root': root,
                'tags': tags,
                'positions': positions,
                'occurrence_count': len(positions)
            }
            merged_data.append(entry)

        print(f"   ‚úì ÿ™ŸÖ ÿØŸÖÿ¨ {len(merged_data):,} ŸÉŸÑŸÖÿ©")

        # 5. Save
        output_file = self.data_dir / 'merged_quran_complete.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ ÿ™ŸÖ ÿßŸÑÿ≠ŸÅÿ∏ ŸÅŸä: {output_file}")

        # 6. Statistics
        print("\n" + "=" * 80)
        print("üìä ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™:")
        print("=" * 80)
        print(f"ŸÉŸÑŸÖÿßÿ™ ŸÅÿ±ŸäÿØÿ©: {len(merged_data):,}")
        print(f"ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑŸÖŸàÿßŸÇÿπ: {total_positions:,}")
        print(f"ŸÖÿ™Ÿàÿ≥ÿ∑ ÿßŸÑÿ™ŸÉÿ±ÿßÿ±: {total_positions / len(merged_data):.2f} ŸÖÿ±ÿ©/ŸÉŸÑŸÖÿ©")

        # Top 10 most frequent
        sorted_data = sorted(merged_data, key=lambda x: x['occurrence_count'], reverse=True)
        print("\nÿ£ŸÉÿ´ÿ± 10 ŸÉŸÑŸÖÿßÿ™ ÿ™ŸÉÿ±ÿßÿ±ÿßŸã:")
        for i, entry in enumerate(sorted_data[:10], 1):
            print(f"  {i:2d}. {entry['word']:15s} - {entry['occurrence_count']:4d} ŸÖÿ±ÿ©")

        return True

if __name__ == '__main__':
    merger = ImprovedMerger()
    merger.merge_with_all_positions()
