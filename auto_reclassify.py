#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ø³ÙƒØ±ÙŠØ¨Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
Auto-Reclassification Script

ÙŠÙØ¹ÙŠØ¯ ØªØµÙ†ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØµÙ†ÙØ© ÙƒÙ€ 6.1 (ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¹Ø¯ Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©
"""

import json
import re
from pathlib import Path
from typing import List, Tuple

class AutoReclassifier:
    def __init__(self, data_dir='qu'):
        self.data_dir = Path(data_dir)

        # Classification rules
        self.rules = self.build_classification_rules()

        # Statistics
        self.stats = {
            'total_words': 0,
            'unknown_before': 0,
            'reclassified': 0,
            'still_unknown': 0
        }

    def build_classification_rules(self):
        """Build classification rules based on map.md"""
        return {
            # 1.0 Ø§Ù„Ø­Ø±ÙˆÙ
            'particles': {
                # Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø±
                'Ù…ÙÙ†': '1.1',
                'ÙÙÙŠ': '1.1',
                'Ø¹ÙÙ„ÙÙ‰': '1.1',
                'Ø¥ÙÙ„ÙÙ‰': '1.1',
                'Ø¨Ù': '1.1',
                'Ù„Ù': '1.1',
                'Ø¹ÙÙ†': '1.1',
                'Ù…ÙØ¹Ù': '1.1',
                'ÙƒÙ': '1.21',  # Ø­Ø±Ù ØªØ´Ø¨ÙŠÙ‡

                # Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø·Ù
                'ÙˆÙ': '1.2',
                'ÙÙ': '1.2',
                'Ø«ÙÙ…ÙÙ‘': '1.2',
                'Ø£ÙÙˆÙ’': '1.2',
                'Ø£ÙÙ…Ù’': '1.2',

                # Ø­Ø±ÙˆÙ Ø§Ù„Ù†ØµØ¨ ÙˆØ§Ù„Ø¬Ø²Ù…
                'Ø£ÙÙ†Ù’': '1.3',
                'Ù„ÙÙ†Ù’': '1.3',
                'Ù„ÙÙ…Ù’': '1.4',
                'Ù„ÙÙ…Ù‘Ø§': '1.4',
                'Ù„ÙÙˆÙ’': '1.4',

                # Ø§Ù„ Ø§Ù„ØªØ¹Ø±ÙŠÙ
                'Ø§Ù„': '1.5',

                # Ø­Ø±Ù Ø§Ù„Ù†Ø¯Ø§Ø¡
                'ÙŠØ§': '1.6',

                # Ø­Ø±ÙˆÙ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
                'Ø£Ù': '1.7',
                'Ù‡ÙÙ„Ù’': '1.7',
                'Ø¡Ù': '1.7',

                # Ø­Ø±Ù Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡
                'Ø¥ÙÙ„Ù‘Ø§': '1.8',

                # Ø­Ø±ÙˆÙ Ø§Ù„ØªÙˆÙƒÙŠØ¯
                'Ø¥ÙÙ†ÙÙ‘': '1.9',
                'Ø£ÙÙ†ÙÙ‘': '1.9',
                'Ù‚ÙØ¯Ù’': '1.9',
                'Ù„ÙÙ€': '1.9',

                # Ø­Ø±ÙˆÙ Ø§Ù„Ù†ÙÙŠ
                'Ù„Ø§': '1.10',
                'Ù…Ø§': '1.10',
                'Ù…ÙØ§': '1.10',

                # Ø­Ø±Ù Ø§Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„
                'Ø³Ù': '1.12',
                'Ø³ÙÙˆÙ’ÙÙ': '1.12',

                # Ø­Ø±Ù Ø§Ù„Ø¥Ø¶Ø±Ø§Ø¨
                'Ø¨ÙÙ„Ù’': '1.14',

                # Ø­Ø±Ù Ø§Ù„Ø§Ø³ØªØ¯Ø±Ø§Ùƒ
                'Ù„Ù°ÙƒÙÙ†Ù’': '1.18',
                'Ù„Ù°ÙƒÙÙ†': '1.18',

                # Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¶Ø§Ø±Ø¹Ø©
                'ÙŠÙ': '1.17',
                'ØªÙ': '1.17',
                'Ù†Ù': '1.17',
                'Ø£Ù': '1.17',
            },

            # 2.0 Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
            'nouns': {
                'Ø§Ù„Ù„Ù‘Ù°Ù‡': '2.2',
                'Ù…ÙÙˆØ³ÙÙ‰': '2.2',
                'Ø¥ÙØ¨Ù’Ø±Ø§Ù‡ÙÙŠÙ…': '2.2',
                'Ø¹ÙÙŠØ³ÙÙ‰': '2.2',
                'Ù…ÙØ­ÙÙ…ÙÙ‘Ø¯': '2.2',
                'Ø¢Ø¯ÙÙ…': '2.2',
                'Ù†ÙÙˆØ­': '2.2',
                'Ø¬ÙÙ‡ÙÙ†ÙÙ‘Ù…': '2.2',

                # Ø£Ø³Ù…Ø§Ø¡ Ø¥Ø´Ø§Ø±Ø©
                'Ù‡Ù°Ø°Ø§': '2.3',
                'Ø°Ù°Ù„ÙÙƒÙ': '2.3',
                'Ø£ÙÙˆÙ„Ù°Ø¦ÙÙƒÙ': '2.3',
                'ØªÙÙ„Ù’ÙƒÙ': '2.3',
                'Ù‡Ù°Ø¤ÙÙ„Ø§Ø¡Ù': '2.3',

                # Ø£Ø³Ù…Ø§Ø¡ Ù…ÙˆØµÙˆÙ„Ø©
                'Ø§Ù„ÙÙ‘Ø°ÙÙŠ': '2.4',
                'Ø§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù': '2.4',
                'Ø§Ù„ÙÙ‘ØªÙÙŠ': '2.4',
                'Ù…ÙÙ†': '2.4',
            },

            # 4.0 Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±
            'pronouns': {
                # Ø¶Ù…Ø§Ø¦Ø± Ù…Ù†ÙØµÙ„Ø©
                'Ù‡ÙÙˆÙ': '4.1',
                'Ù‡ÙÙ…': '4.1',
                'Ù‡ÙÙ…Ù’': '4.1',
                'Ø£ÙÙ†Ù’ØªÙÙ…': '4.1',
                'Ø£ÙÙ†Ø§': '4.1',
                'Ù†ÙØ­Ù’Ù†Ù': '4.1',
                'Ù‡ÙÙŠÙ': '4.1',
                'Ø£ÙÙ†Ù’ØªÙ': '4.1',

                # Ø¶Ù…Ø§Ø¦Ø± Ù…ØªØµÙ„Ø©
                'Ù‡Ù': '4.2',
                'Ù‡Ù': '4.2',
                'Ù‡ÙÙ…': '4.2',
                'Ù‡ÙÙ…Ù': '4.2',
                'ÙƒÙ': '4.2',
                'ÙƒÙÙ…': '4.2',
                'ÙƒÙÙ…Ù’': '4.2',
                'Ù†Ø§': '4.2',
                'ÙÙŠ': '4.2',
                'Ù‡Ø§': '4.2',
                'ÙÙˆØ§': '4.2',
                'ØªÙÙ…': '4.2',
                'ØªÙÙ…Ù’': '4.2',
                'ØªÙ': '4.2',
            },

            # 5.0 Ø§Ù„Ø¸Ø±ÙˆÙ
            'adverbs': {
                'Ø¥ÙØ°Ø§': '5.1',
                'Ø¥ÙØ°Ù’': '5.1',
                'Ø¨ÙØ¹Ù’Ø¯': '5.1',
                'Ù‚ÙØ¨ÙÙ„': '5.1',
                'ÙŠÙÙˆÙ’Ù…ÙØ¦ÙØ°Ù': '5.1',
                'Ø­ÙÙŠÙ†': '5.1',

                'Ø¹ÙÙ†Ù’Ø¯': '5.2',
                'Ø«ÙÙ…ÙÙ‘': '5.2',
                'ÙÙÙˆÙ’Ù‚': '5.2',
                'ØªÙØ­Ù’Øª': '5.2',
                'Ø¨ÙÙŠÙ’Ù†Ù': '5.2',
                'Ø¯ÙÙˆÙ†': '5.2',
            }
        }

    def classify_morpheme(self, morpheme: str, root: str = None) -> str:
        """Classify a single morpheme"""
        # Remove diacritics for comparison
        clean = self.remove_diacritics(morpheme)

        # Check particles
        for word, tag in self.rules['particles'].items():
            if self.remove_diacritics(word) == clean or morpheme == word:
                return tag

        # Check nouns
        for word, tag in self.rules['nouns'].items():
            if self.remove_diacritics(word) == clean or morpheme == word:
                return tag

        # Check pronouns
        for word, tag in self.rules['pronouns'].items():
            if self.remove_diacritics(word) == clean or morpheme == word:
                return tag

        # Check adverbs
        for word, tag in self.rules['adverbs'].items():
            if self.remove_diacritics(word) == clean or morpheme == word:
                return tag

        # Heuristic rules based on patterns

        # If has root and 3+ letters, likely a verb or noun
        if root and root not in ['NTWS', 'Ø­Ø±Ù', 'Ø¸Ø±Ù'] and len(clean) >= 3:
            # Check if it starts with verb prefixes
            if morpheme.startswith(('ÙŠÙ', 'ØªÙ', 'Ù†Ù', 'Ø£Ù')) and len(clean) >= 4:
                return '3.2'  # ÙØ¹Ù„ Ù…Ø¶Ø§Ø±Ø¹

            # Check past tense patterns (typically ends with Ù or no suffix)
            if re.search(r'[Ù]$', morpheme):
                return '3.1'  # ÙØ¹Ù„ Ù…Ø§Ø¶Ù

        # Default: keep as unknown
        return '6.1'

    def remove_diacritics(self, text: str) -> str:
        """Remove Arabic diacritics"""
        arabic_diacritics = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
        return arabic_diacritics.sub('', text)

    def reclassify_word(self, word: str, root: str, tags: str) -> str:
        """Reclassify a word entry"""
        # If not unknown, keep as is
        if tags != '6.1':
            return tags

        # Split word into morphemes
        if '+' in word:
            morphemes = word.split('+')
        else:
            morphemes = [word]

        # Classify each morpheme
        new_tags = []
        for morpheme in morphemes:
            tag = self.classify_morpheme(morpheme, root)
            new_tags.append(tag)

        return ','.join(new_tags)

    def process_files(self):
        """Process all final_XX.json files and reclassify"""
        print("=" * 80)
        print("ğŸ”„ Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ")
        print("=" * 80)

        for i in range(1, 16):
            file_num = f'{i:02d}'
            input_file = self.data_dir / 'final' / f'final_{file_num}.json'

            if not input_file.exists():
                print(f"âš  Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {input_file}")
                continue

            print(f"\nğŸ“‚ Ù…Ø¹Ø§Ù„Ø¬Ø© {input_file.name}...")

            # Load data
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Reclassify
            reclassified_data = []
            file_unknown_before = 0
            file_reclassified = 0

            for entry in data:
                if len(entry) < 3:
                    reclassified_data.append(entry)
                    continue

                word, root, tags = entry[0], entry[1], entry[2]

                self.stats['total_words'] += 1

                if tags == '6.1':
                    file_unknown_before += 1
                    self.stats['unknown_before'] += 1

                    # Reclassify
                    new_tags = self.reclassify_word(word, root, tags)

                    if new_tags != '6.1':
                        file_reclassified += 1
                        self.stats['reclassified'] += 1
                        reclassified_data.append([word, root, new_tags])
                    else:
                        self.stats['still_unknown'] += 1
                        reclassified_data.append(entry)
                else:
                    reclassified_data.append(entry)

            # Save
            output_file = self.data_dir / 'final' / f'final_{file_num}_reclassified.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(reclassified_data, f, ensure_ascii=False, indent=2)

            print(f"  âœ“ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ Ù‚Ø¨Ù„: {file_unknown_before}")
            print(f"  âœ“ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØµÙ†ÙŠÙ: {file_reclassified}")
            print(f"  âœ“ Ø­ÙÙØ¸ ÙÙŠ: {output_file.name}")

    def print_report(self):
        """Print final report"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
        print("=" * 80)
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {self.stats['total_words']:,}")
        print(f"ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ Ù‚Ø¨Ù„: {self.stats['unknown_before']:,} ({(self.stats['unknown_before']/self.stats['total_words']*100):.2f}%)")
        print(f"ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØµÙ†ÙŠÙ: {self.stats['reclassified']:,} ({(self.stats['reclassified']/self.stats['total_words']*100):.2f}%)")
        print(f"Ù…Ø§ Ø²Ø§Ù„ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {self.stats['still_unknown']:,} ({(self.stats['still_unknown']/self.stats['total_words']*100):.2f}%)")
        print("=" * 80)

    def run(self):
        """Run reclassification"""
        self.process_files()
        self.print_report()


if __name__ == '__main__':
    classifier = AutoReclassifier()
    classifier.run()
